# Multimodal 

**Toward Self-Supervised Object Detection in Unlabeled Videos**  
*Elad Amrani, Rami Ben-Ari, Tal Hakim, Alex Bronstein*  
[[paper](https://arxiv.org/abs/1905.11137)]  

**Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding (CVPR2019)**  
*Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, Shih-Fu Chang*  
[[paper](https://arxiv.org/abs/1811.11683)]  

**FAN: Focused Attention Networks**  
*Chu Wang, Babak Samari, Vladimir Kim, Siddhartha Chaudhuri, Kaleem Siddiqi*  
[[paper](https://arxiv.org/abs/1905.11498)]  

**Learning to Explain with Complemental Examples (CVPR2019)**  
*Atsushi Kanehira, Tatsuya Harada*  
[[paper](https://arxiv.org/abs/1812.01280)]  

**Sketchforme: Composing Sketched Scenes from Text Descriptions for Interactive Applications**  
*Forrest Huang, John F. Canny*  
[[paper](https://arxiv.org/abs/1904.04399)]  

**End-to-End Learning Using Cycle Consistency for Image-to-Caption Transformations**  
*Keisuke Hagiwara, Yusuke Mukuta, Tatsuya Harada*  
[[paper](https://arxiv.org/abs/1903.10118)]  

**Text-to-Image-to-Text Translation using Cycle Consistent Adversarial Networks**  
*Satya Krishna Gorti, Jeremy Ma*  
[[paper](https://arxiv.org/abs/1808.04538)]  

**Talking Face Generation by Adversarially Disentangled Audio-Visual Representation (AAAI2019 oral)**  
*Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang*  
[[paper](https://arxiv.org/abs/1807.07860)]  

**Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations (CVPR2019)**  
*Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, Wei-Ying Ma*  
[[paper](https://arxiv.org/abs/1904.05521)]  

**Prospection: Interpretable Plans From Language By Predicting the Future (ICRA2019)**  
*Chris Paxton, Yonatan Bisk, Jesse Thomason, Arunkumar Byravan, Dieter Fox*  
[[paper](https://arxiv.org/abs/1903.08309)]  

**A Knowledge-Grounded Multimodal Search-Based Conversational Agent**  
*Shubham Agarwal, Ondrej Dusek, Ioannis Konstas, Verena Rieser*  
[[paper](https://arxiv.org/abs/1810.11954)]  

**Hierarchical Scene Parsing by Weakly Supervised Learning with Image Descriptions**  
*Ruimao Zhang, Liang Lin, Guangrun Wang, Meng Wang, Wangmeng Zuo*  
[[paper](https://arxiv.org/abs/1709.09490)]  

**Learning Robust Visual-Semantic Embeddings (ICCV2017)**  
[[paper](http://openaccess.thecvf.com/content_ICCV_2017/papers/Tsai_Learning_Robust_Visual-Semantic_ICCV_2017_paper.pdf)]  

**Deep Visual-Semantic Quantization for Efficient Image Retrieval (CVPR2017)**  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Deep_Visual-Semantic_Quantization_CVPR_2017_paper.pdf)]  

**Transductive Visual-Semantic Embedding for Zero-shot Learning (ICMR2017)**  
[[paper](https://dl.acm.org/citation.cfm?id=3078977)]  

**Hierarchical Multimodal LSTM for Dense Visual-Semantic Embedding (ICCV2017)**  
[[paper](http://www.ganghua.org/publication/ICCV17c.pdf)]  

**Multiple Instance Visual-Semantic Embedding (BMVC2017)**  
[[paper](http://web.cs.ucla.edu/~zhou.ren/Zhou_bmvc17_paper.pdf)]  

**Finding beans in burgers: Deep semantic-visual embedding with localization (CVPR2018)**   
[[paper](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3272.pdf)]  

**Fine-grained Image Classification by Visual-Semantic Embedding (IJCAI2018)**  
[[paper](https://www.ijcai.org/proceedings/2018/0145.pdf)]  

**VSE-ens: Visual-Semantic Embeddings with Efficient Negative Sampling (AAAI2018)**  
*Guibing Guo, Songlin Zhai, Fajie Yuan, Yuan Liu, Xingwei Wang*  
[[paper](https://arxiv.org/abs/1801.01632)]  

**VSE++: Improving Visual-Semantic Embeddings with Hard Negatives (BMVC2018 spotlight)**  
*Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, Sanja Fidler*  
[[paper](https://arxiv.org/abs/1707.05612)]  

**Actor and Action Video Segmentation from a Sentence (CVPR2018 oral)**  
*Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, Cees G.M. Snoek*  
[[paper](https://arxiv.org/abs/1803.07485)]  

**Guide Me: Interacting with Deep Networks**  
*Christian Rupprecht, Iro Laina, Nassir Navab, Gregory D. Hager, Federico Tombari*  
[[paper](https://arxiv.org/abs/1803.11544)]  

**Multimodal Explanations: Justifying Decisions and Pointing to the Evidence**  
*Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, Marcus Rohrbach*  
[[paper](https://arxiv.org/abs/1802.08129)]  

**From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood(ACL2017)**  
[[paper](http://aclweb.org/anthology/P/P17/P17-1097.pdf)]  

**Gated-Attention Architectures for Task-Oriented Language Grounding**  
*Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov*  
[[paper](https://arxiv.org/abs/1706.07230)]  

**Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures(WWW2018)**  
*Wen Hua Lin, Kuan-Ting Chen, Hung Yueh Chiang, Winston Hsu*  
[[paper](https://arxiv.org/abs/1801.10300)]  

**Natural Language Communication with Robots (NAACL2016)**  
[[paper](http://www.aclweb.org/anthology/N16-1089)]  

**Source-Target Inference Models for Spatial Instruction Understanding (AAAI2018)**  
*Hao Tan, Mohit Bansal*  
[[paper](https://arxiv.org/abs/1707.03804)]  

**Learning Interpretable Spatial Operations in a Rich 3D Blocks World (AAAI2018)**  
*Yonatan Bisk, Kevin J. Shih, Yejin Choi, Daniel Marcu*  
[[paper](https://arxiv.org/abs/1712.03463)]  

**Grounded Language Learning in a Simulated 3D World**  
*Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, Phil Blunsom*  
[[paper](https://arxiv.org/abs/1706.06551)]  

**Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions**  
*Jun Hatori, Yuta Kikuchi, Sosuke Kobayashi, Kuniyuki Takahashi, Yuta Tsuboi, Yuya Unno, Wilson Ko, Jethro Tan*  
[[paper](https://arxiv.org/abs/1710.06280)]  

**Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings(NAACL2016)**  
[[paper](http://www.aclweb.org/anthology/N16-1022)]  

**Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes(EMNLP2016)**  
[[paper](http://www.aclweb.org/anthology/D16-1156)]  

**Learning Sight from Sound: Ambient Sound Provides Supervision for Visual Learning**  
*Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman, Antonio Torralba*  
[[paper](https://arxiv.org/abs/1712.07271v1)]  

**Learning Modality-Invariant Representations for Speech and Images**  
*Kenneth Leidal, David Harwath, James Glass*  
[[paper](https://arxiv.org/abs/1712.03897)]  

**Visual to Sound: Generating Natural Sound for Videos in the Wild**  
[[project](http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html)]  

**Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments**  
*Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton van den Hengel*  
[[paper](https://arxiv.org/abs/1711.07280)]  

**Person Search with Natural Language Description**  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Person_Search_With_CVPR_2017_paper.pdf)]  

**From Red Wine to Red Tomato: Composition with Context**  
[[paper](http://www.cs.cmu.edu/~imisra/data/composing_cvpr17.pdf)]  

**VSE++: Improved Visual-Semantic Embeddings**  
*Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, Sanja Fidler*  
[[paper](https://arxiv.org/abs/1707.05612)]  

**Self-supervised learning of visual features through embedding images into text topic spaces(CVPR2017)**  
[[paper](https://arxiv.org/pdf/1705.08631.pdf)]  
[[code](https://github.com/lluisgomez/TextTopicNet)]  

**SCAN: Learning Abstract Hierarchical Compositional Visual Concepts**  
*Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matthew Botvinick, Demis Hassabis, Alexander Lerchner*  
[[paper](https://arxiv.org/abs/1707.03389)]  

**Conditional generation of multi-modal data using constrained embedding space mapping**  
*Subhajit Chaudhury, Sakyasingha Dasgupta, Asim Munawar, Md. A. Salam Khan, Ryuki Tachibana*  
[[paper](https://arxiv.org/abs/1707.00860)]  

**Look, Listen and Learn**  
*Relja Arandjelović, Andrew Zisserman*  
[[paper](https://arxiv.org/abs/1705.08168)]  

**Recurrent Multimodal Interaction for Referring Image Segmentation**  
*Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Alan Yuille*  
[[paper](https://arxiv.org/abs/1703.07939)]  

**Visually grounded learning of keyword prediction from untranscribed speech**  
*Herman Kamper, Shane Settle, Gregory Shakhnarovich, Karen Livescu*  
[[paper](https://arxiv.org/abs/1703.08136v1)]  

**Cross-modal Deep Metric Learning with Multi-task Regularization**  
*Xin Huang, Yuxin Peng*  
[[paper](https://arxiv.org/abs/1703.07026)]  

**Fusion of EEG and Musical Features in Continuous Music-emotion Recognition(AAAI2017)**  
*Nattapong Thammasan, Ken-ichi Fukui, Masayuki Numao*  
[[paper](https://arxiv.org/abs/1611.10120)]  

